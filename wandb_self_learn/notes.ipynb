{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [lecture_1 notes](Intro_to_Weights_&_Biases.ipynb)\n",
    "## <span style=\"color:yellow\"> how to initialize a wandb board: the project name, run name </span>\n",
    "## <span style=\"color:yellow\"> how to add the scalar to the tensorboard </span>\n",
    "## <span style=\"color:yellow\"> how to create the metric (combine multiple characteristic together)</span>\n",
    "## <span style=\"color:yellow\"> how to send the alert</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## wandb.init()\n",
    "```python \n",
    "    wandb.init(\n",
    "        project=\"name of the whole project\",\n",
    "        name = \"specific one run name\" # since there will be multipe run inside the same project name (change the lr)\n",
    "        config={\n",
    "            #define the parameters related to model training here such as lr, batch_size, dropout rate...\n",
    "        }\n",
    "        notes=\"give a breif explaination of this run\"\n",
    "        tages = [\"help to search\"]\n",
    "    )\n",
    "\n",
    "    ##code example:\n",
    "    wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"basic-intro\",\n",
    "    # We pass a run name (otherwise itâ€™ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=f\"experiment_{run}\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    })\n",
    "\n",
    "```\n",
    "- ## wandb.log()\n",
    "    - copy the certain data to the wandb board, and finally after several epock you can see a plot on the board \n",
    "```python \n",
    "    ## example code:\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "    \n",
    "    # acc and loss are the name of the plot \n",
    "```\n",
    "- ## create metrics and log then to the wandb board \n",
    "```python\n",
    "#define the metrisc\n",
    "    metrics = {\"train/train_loss\": train_loss,\n",
    "                       \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n",
    "                       \"train/example_ct\": example_ct}\n",
    "#add the metrics\n",
    "    wandb.log(metrics)\n",
    "```\n",
    "- ## dictionary unpackage: add two dictionary together:\n",
    "```python\n",
    "    wandb.log({**metrics, **val_metrics})\n",
    "```\n",
    "\n",
    "- ## add the alert of wandb (you will receive the email if the alert was triggered)\n",
    "```python\n",
    "if accuracy <= acc_threshold:\n",
    "        # ðŸ Send the wandb Alert\n",
    "        wandb.alert(\n",
    "            title='Low Accuracy', # this will be the name of your emial \n",
    "            text=f'Accuracy {accuracy} at step {training_step} is below the acceptable theshold, {acc_threshold}',# this will be the text of that email \n",
    "        )\n",
    "        print('Alert triggered') # this will be print out on your local terminal \n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [lecture_2 notes](W&B_Tables_Quickstart.ipynb)\n",
    "## <span style=\"color:yellow\">teach how to show a table in the wandb board </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## How to create wandb table \n",
    "```python\n",
    "#step one: call the Table() helper function from wandb \n",
    "    #define the name of the column \n",
    "    columns=[\"id\", \"image\", \"guess\", \"truth\"]\n",
    "    for digit in range(10):\n",
    "      columns.append(\"score_\" + str(digit))\n",
    "\n",
    "    #call the Table() function \n",
    "    test_table = wandb.Table(columns=colums) # here you need to setup the name of each column \n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#step two: add the contents to the table \n",
    "    test_table.add_data(img_id, wandb.Image(i), p, l, *s) # the data you add should follow the order of the column name\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#step three: log the table to the website:\n",
    "    wandb.log({\"test_predictions\" : test_table})\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "#code example:\n",
    "def log_test_predictions(images, labels, outputs, predicted, test_table, log_counter):\n",
    "  # obtain confidence scores for all classes\n",
    "  scores = F.softmax(outputs.data, dim=1)\n",
    "  log_scores = scores.cpu().numpy()\n",
    "  log_images = images.cpu().numpy()\n",
    "  log_labels = labels.cpu().numpy()\n",
    "  log_preds = predicted.cpu().numpy()\n",
    "  # adding ids based on the order of the images\n",
    "  _id = 0\n",
    "  for i, l, p, s in zip(log_images, log_labels, log_preds, log_scores):\n",
    "    # add required info to data table:\n",
    "    # id, image pixels, model's guess, true label, scores for all classes\n",
    "    img_id = str(_id) + \"_\" + str(log_counter)\n",
    "    test_table.add_data(img_id, wandb.Image(i), p, l, *s)\n",
    "    _id += 1\n",
    "    if _id == NUM_IMAGES_PER_BATCH:\n",
    "      break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [lecture 3](Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W&B.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:yellow\">How to let wandb to automatically search through combinations of hyperparameter values to find values that optimizes your model</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 steps to run a hyperparameter sweep:\n",
    "\n",
    "1. ## **Define the sweep:** we do this by creating a dictionary or a [YAML file](https://docs.wandb.com/library/sweeps/configuration) that specifies the parameters to search through, the search strategy, the optimization metric et all.\n",
    "\n",
    "    - 1. ### pick a search method:\n",
    "        - grid \n",
    "        - random \n",
    "        - Bayesian \n",
    "    ```python\n",
    "    sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "    ```\n",
    "    - 2. ### specify the metric that you want to optimize for\n",
    "    ```python\n",
    "    metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize'\n",
    "    }\n",
    "\n",
    "    sweep_config['metric'] = metric\n",
    "    ```\n",
    "    - 3. ### specify hyperparameters to search throught \n",
    "        - case1: constant value \n",
    "        ```python\n",
    "        parameters_dict.update({\n",
    "        'epochs': {\n",
    "            'value': 1}\n",
    "        })\n",
    "        ```\n",
    "        - case2: list out several numbers\n",
    "        ```python\n",
    "        parameters_dict = {\n",
    "            'optimizer': {\n",
    "                'values': ['adam', 'sgd']\n",
    "                },\n",
    "            'fc_layer_size': {\n",
    "                'values': [128, 256, 512]\n",
    "                },\n",
    "            'dropout': {\n",
    "                'values': [0.3, 0.4, 0.5]\n",
    "                },\n",
    "        }\n",
    "        sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "        ```\n",
    "        - case3: give a distribution and set the min, max ...\n",
    "        ```python\n",
    "        parameters_dict.update({\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0 and 0.1\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'batch_size': {\n",
    "            # integers between 32 and 256\n",
    "            # with evenly-distributed logarithms\n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'q': 8,\n",
    "            'min': 32,\n",
    "            'max': 256,\n",
    "        }\n",
    "        })\n",
    "        ```\n",
    "\n",
    "\n",
    "2. ## **Initialize the sweep:** with one line of code we initialize the sweep and pass in the dictionary of sweep configurations\n",
    "`sweep_id = wandb.sweep(sweep_config)`\n",
    "- ### we will use the W&B controller to manage sweeps\n",
    "- ### the component that actually executes a sweep is known as a sweep agent \n",
    "```python \n",
    "# you will need to use this sweep_id later\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n",
    "```\n",
    "\n",
    "\n",
    "3. ## **Run the sweep agent:** also accomplished with one line of code, we call `wandb.agent()` and pass the `sweep_id` to run, along with a function that defines your model architecture and trains it:\n",
    "`wandb.agent(sweep_id, function=train)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
