{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 10\n",
    "W = 10\n",
    "focal = 138\n",
    "near = 2\n",
    "far = 6\n",
    "n_sample = 5\n",
    "# This pose is the camera facing the -x direction \n",
    "pose = torch.tensor([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [-1, 0, 0, -1],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "negative_z_pose = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, -1],\n",
    "    [0, 0, 0, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yuzhen_get_rays(pose, H, W, focal):\n",
    "    #the input is:\n",
    "        # 1. the focal length (use the to calculate the relative direction of the ray according to the camera)\n",
    "        # 2. the 4*4 camera position matreix:\n",
    "            # contain the 3*3 rotation matrix\n",
    "            # 3*1 transfomation matrix\n",
    "        # 3. the size of the frame (pixel as the unit in order to define the ray number)\n",
    "\n",
    "    #the output is:\n",
    "        # 1. the original of the rays\n",
    "        # 2. the direction of the rays\n",
    "\n",
    "\n",
    "    R = torch.zeros([W])\n",
    "    for i in range(int(-W*0.5),int(W*0.5)):\n",
    "        R[int(i+W*0.5)] = i\n",
    "    R = torch.unsqueeze(R,0) # create another row dimention\n",
    "    R = R.expand(H,-1)\n",
    "\n",
    "\n",
    "    C = torch.zeros([H])\n",
    "    for i in range(int(-H/2), int(H/2)):\n",
    "        C[int(i+int(H/2))] = -i\n",
    "    C = torch.unsqueeze(C,1)\n",
    "    # print(C)\n",
    "    C = C.expand(-1,W)\n",
    "\n",
    "    R = torch.unsqueeze(R,2)\n",
    "    C = torch.unsqueeze(C,2)\n",
    "    R_C = torch.cat((R, C), 2)\n",
    "    z_dim = torch.ones(R.shape)\n",
    "    z_dim = z_dim * -focal\n",
    "    R_C_Z = torch.cat((R_C, z_dim),2)\n",
    "    R_C_Z = R_C_Z / focal\n",
    "    rotation_matrix = pose[:3, :3]\n",
    "\n",
    "    R_C_Z = torch.unsqueeze(R_C_Z,2) # change the shape from [10,10,3] to [10,10,1,3] used for the later matiplication\n",
    "    real_R_C_Z = R_C_Z * rotation_matrix # [10,10,1,3] * [3*3] --> [10,10,1,3] * [1,1,3,3] = [10,10,3,3]\n",
    "    real_R_C_Z = torch.sum(real_R_C_Z,-1) # add the colum of the 3*3 matrix together\n",
    "\n",
    "    rays_direction = real_R_C_Z\n",
    "    rays_direction = F.normalize(rays_direction, p=2, dim=2)\n",
    "    rays_origion = pose[:3, -1]\n",
    "    rays_origion = rays_origion.expand(rays_direction.shape)\n",
    "\n",
    "    return rays_origion, rays_direction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# R = torch.zeros([W])\n",
    "# for i in range(int(-W*0.5),int(W*0.5)):\n",
    "#     R[int(i+W*0.5)] = i\n",
    "# # print(R)\n",
    "\n",
    "# R = torch.unsqueeze(R,0) # create another row dimention\n",
    "# # print(R)\n",
    "# # print(R.shape)\n",
    "# R = R.expand(H,-1)\n",
    "# # print(R)\n",
    "# # print(R.shape)\n",
    "\n",
    "\n",
    "# C = torch.zeros([H])\n",
    "# for i in range(int(-H/2), int(H/2)):\n",
    "#     C[int(i+int(H/2))] = -i\n",
    "# C = torch.unsqueeze(C,1)\n",
    "# # print(C)\n",
    "# C = C.expand(-1,W)\n",
    "# # print(C)\n",
    "\n",
    "# R = torch.unsqueeze(R,2)\n",
    "# print(R.shape)\n",
    "# C = torch.unsqueeze(C,2)\n",
    "# print(C.shape)\n",
    "# R_C = torch.cat((R, C), 2)\n",
    "# print(R_C.shape)\n",
    "# # print(R_C)\n",
    "\n",
    "# z_dim = torch.ones(R.shape)\n",
    "# z_dim = z_dim * -focal\n",
    "# print(z_dim.shape)\n",
    "# # print(z_dim)\n",
    "\n",
    "# R_C_Z = torch.cat((R_C, z_dim),2)\n",
    "# print(R_C_Z.shape)\n",
    "# R_C_Z = R_C_Z / focal\n",
    "# # print (R_C_Z)\n",
    "\n",
    "\n",
    "# rotation_matrix = pose[:3, :3]\n",
    "\n",
    "# ## one direction for test demo--------\n",
    "# test_dir = R_C_Z[0][0]\n",
    "# print(test_dir)\n",
    "\n",
    "\n",
    "# real_test_dir = test_dir * rotation_matrix\n",
    "# print(rotation_matrix)\n",
    "# print (real_test_dir)\n",
    "# real_test_dir = torch.sum(real_test_dir, -1)\n",
    "# print(real_test_dir)\n",
    "# ##--------------------------------------\n",
    "\n",
    "# ## do it for all the direction---------\n",
    "# R_C_Z = torch.unsqueeze(R_C_Z,2) # change the shape from [10,10,3] to [10,10,1,3] used for the later matiplication\n",
    "# print(R_C_Z.shape)\n",
    "# real_R_C_Z = R_C_Z * rotation_matrix # [10,10,1,3] * [3*3] --> [10,10,1,3] * [1,1,3,3] = [10,10,3,3]\n",
    "# real_R_C_Z = torch.sum(real_R_C_Z,-1) # add the colum of the 3*3 matrix together\n",
    "# print(real_R_C_Z.shape)\n",
    "# # print(real_R_C_Z)\n",
    "# ##-------------------------------------\n",
    "\n",
    "\n",
    "# rays_direction = real_R_C_Z\n",
    "\n",
    "# ##-----------------------start to calculate the rays_original-------------------------------\n",
    "# ## the origin of the rays is the camera position --> the transportation 3D vector\n",
    "# rays_origion = pose[:3, -1] # the :3 means the row 0,1,2. the -1 means the last colum\n",
    "# print(rays_origion.shape)\n",
    "# # print(rays_origion)\n",
    "\n",
    "# ## since later we need to use the rays_direction and rays_origion to do the calculation, so it's better\n",
    "# ## to have them have the shape shape\n",
    "# #rays_direction shape --> [10,10,3]\n",
    "# #rays_origion shape --> [3]\n",
    "\n",
    "# rays_origion = rays_origion.expand(rays_direction.shape)\n",
    "# print(rays_origion.shape)\n",
    "# print(rays_origion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39])\n",
      "tensor([ 1.0000,  2.0000,  3.0000,  0.8415,  0.9093,  0.1411,  0.5403, -0.4161,\n",
      "        -0.9900,  0.9093, -0.7568, -0.2794, -0.4161, -0.6536,  0.9602, -0.7568,\n",
      "         0.9894, -0.5366, -0.6536, -0.1455,  0.8439,  0.9894, -0.2879, -0.9056,\n",
      "        -0.1455, -0.9577,  0.4242, -0.2879,  0.5514, -0.7683, -0.9577,  0.8342,\n",
      "        -0.6401,  0.5514,  0.9200,  0.9836,  0.8342,  0.3919, -0.1804])\n"
     ]
    }
   ],
   "source": [
    "def yuzhen_position_encoder(original_position, embeded_level):\n",
    "    #the input:\n",
    "        #the original position \n",
    "        #max level for encoding\n",
    "    #the output:\n",
    "        #is a list, with (#embeded_level * 2 + 1) elements inside\n",
    "    encoded_position = [original_position]\n",
    "    for i in range (0,embeded_level):\n",
    "        temp_cos = torch.cos(2**i * original_position)\n",
    "        temp_sin = torch.sin(2**i * original_position)\n",
    "        encoded_position.append(temp_sin)\n",
    "        encoded_position.append(temp_cos)\n",
    "\n",
    "    encoded_position_torch = torch.cat(encoded_position, -1)        \n",
    "    return encoded_position_torch\n",
    "\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "result = yuzhen_position_encoder(x, 6)\n",
    "print(result.shape)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39])\n",
      "tensor([ 1.0000,  2.0000,  3.0000,  0.8415,  0.9093,  0.1411,  0.5403, -0.4161,\n",
      "        -0.9900,  0.9093, -0.7568, -0.2794, -0.4161, -0.6536,  0.9602, -0.7568,\n",
      "         0.9894, -0.5366, -0.6536, -0.1455,  0.8439,  0.9894, -0.2879, -0.9056,\n",
      "        -0.1455, -0.9577,  0.4242, -0.2879,  0.5514, -0.7683, -0.9577,  0.8342,\n",
      "        -0.6401,  0.5514,  0.9200,  0.9836,  0.8342,  0.3919, -0.1804])\n"
     ]
    }
   ],
   "source": [
    "def positional_encoder(x, L_embed=6):\n",
    "  \"\"\"\n",
    "  This function applies positional encoding to the input tensor. Positional encoding is used in NeRF\n",
    "  to allow the model to learn high-frequency details more effectively. It applies sinusoidal functions\n",
    "  at different frequencies to the input.\n",
    "\n",
    "  Parameters:\n",
    "  x (torch.Tensor): The input tensor to be positionally encoded.\n",
    "  L_embed (int): The number of frequency levels to use in the encoding. Defaults to 6.\n",
    "\n",
    "  Returns:\n",
    "  torch.Tensor: The positionally encoded tensor.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize a list with the input tensor.\n",
    "  rets = [x]\n",
    "\n",
    "  # Loop over the number of frequency levels.\n",
    "  for i in range(L_embed):\n",
    "    #############################################################################\n",
    "    #                                   TODO                                    #\n",
    "    #############################################################################\n",
    "    sin_encoding = torch.sin(2.0 ** i * x)\n",
    "    cos_encoding = torch.cos(2.0 ** i * x)\n",
    "    rets.extend([sin_encoding, cos_encoding])\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "\n",
    "  # Concatenate the original and encoded features along the last dimension.\n",
    "  return torch.cat(rets, -1)\n",
    "\n",
    "\n",
    "x = torch.tensor([1, 2, 3])\n",
    "result = positional_encoder(x, 6)\n",
    "print(result.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the nerf model \n",
    "- we use the fully connection linear layers here (3 layers for the tiny model)\n",
    "- use the relu function as the activate function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mini_nerf_model(torch.nn.Module):\n",
    "    # def __init__(self, filter_size = 128, embeded_level = 6):\n",
    "    #     super(mini_nerf_model, self).__init__()\n",
    "\n",
    "    #     self.layer1 = torch.nn.Linear(3+3*2*embeded_level, filter_size)\n",
    "    #     self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "    #     self.layer3 = torch.nn.Linear(filter_size, 3+1)\n",
    "    #     self.relu = F.relu\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.relu(self.layer1(x))\n",
    "    #     x = self.relu(self.layer2(x))\n",
    "    #     x = self.layer3(x)\n",
    "    #     return x\n",
    "\n",
    "    def __init__(self, filter_size=128, num_encoding_functions=6):\n",
    "        super(mini_nerf_model, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "        self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "# get the rays_origion and rays_direction \n",
    "rays_origion, rays_direction = yuzhen_get_rays(negative_z_pose,H, W, focal)\n",
    "print(rays_direction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([1, 1, 5, 1])\n",
      "torch.Size([10, 10, 5, 1])\n",
      "torch.Size([10, 10, 1, 1, 1, 1, 1, 1, 1, 1, 3])\n",
      "torch.Size([10, 10, 1, 1, 1, 1, 1, 10, 10, 5, 3])\n",
      "torch.Size([10, 10, 1, 1, 1, 1, 1, 10, 10, 5, 39])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for mini_nerf_model:\n\tMissing key(s) in state_dict: \"layer1.weight\", \"layer1.bias\", \"layer2.weight\", \"layer2.bias\", \"layer3.weight\", \"layer3.bias\". \n\tUnexpected key(s) in state_dict: \"module.layer1.weight\", \"module.layer1.bias\", \"module.layer2.weight\", \"module.layer2.bias\", \"module.layer3.weight\", \"module.layer3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m nerf_model \u001b[38;5;241m=\u001b[39m mini_nerf_model()\n\u001b[1;32m     62\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 63\u001b[0m \u001b[43mnerf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m raw_result \u001b[38;5;241m=\u001b[39m nerf_model(all_points_encoded[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_result\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/ENTER/envs/RSS/lib/python3.8/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for mini_nerf_model:\n\tMissing key(s) in state_dict: \"layer1.weight\", \"layer1.bias\", \"layer2.weight\", \"layer2.bias\", \"layer3.weight\", \"layer3.bias\". \n\tUnexpected key(s) in state_dict: \"module.layer1.weight\", \"module.layer1.bias\", \"module.layer2.weight\", \"module.layer2.bias\", \"module.layer3.weight\", \"module.layer3.bias\". "
     ]
    }
   ],
   "source": [
    "def yuzhen_render(nerf_moel, rays_direction, rays_origion, near, far, n_sample):\n",
    "    #the input is \n",
    "        # 1. the camera position\n",
    "        # 2. ray direction \n",
    "        # 3. the near point \n",
    "        # 4. the far point\n",
    "        # 5. the sample point number\n",
    "    \n",
    "    #the overfitted mode: imported\n",
    "\n",
    "    #the output is \n",
    "        # 1. the density at each point\n",
    "        # 2. the RGB color at each point \n",
    "\n",
    "    #algorithm: \n",
    "    #1. \n",
    "        # the nerf itself only need the encoded position as the input\n",
    "        # the output of the nerf model will be the rgb value and density value of each point\n",
    "    #2. \n",
    "        # once we have the rgb an density value of each point, we need to use the camera location and angle to render the frame\n",
    "        # calculate weight and alpha \n",
    "    \n",
    "    #step1: calculate the points coordinates \n",
    "    distance = torch.linspace(near, far, n_sample) \n",
    "    distance = distance.reshape(1,1, n_sample,1)\n",
    " \n",
    "    distance = distance.expand(W,H,-1,1)\n",
    " \n",
    "    rays_direction = torch.unsqueeze(rays_direction, 2)\n",
    " \n",
    "    all_points = distance * rays_direction\n",
    "\n",
    "    raw_outcome = nerf_moel()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "distance = torch.linspace(near, far, n_sample) \n",
    "print(distance.shape)\n",
    "distance = distance.reshape(1,1, n_sample,1)\n",
    "print(distance.shape)\n",
    "\n",
    "distance = distance.expand(W,H,-1,1)\n",
    "print(distance.shape)\n",
    "\n",
    "rays_direction = torch.unsqueeze(rays_direction, 2)\n",
    "print(rays_direction.shape)\n",
    "\n",
    "all_points = distance * rays_direction\n",
    "print(all_points.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_points_encoded = yuzhen_position_encoder(all_points, embeded_level=6)\n",
    "# print(all_points_encoded)\n",
    "print(all_points_encoded.shape)\n",
    "\n",
    "nerf_model = mini_nerf_model()\n",
    "ckpt = torch.load(\"pretrained.pth\", map_location=torch.device(\"cpu\"))\n",
    "print(ckpt)\n",
    "nerf_model.load_state_dict(ckpt)\n",
    "raw_result = nerf_model(all_points_encoded[0][0][0])\n",
    "print(raw_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 3])\n",
      "tensor([[[-1.0000,  0.0362,  0.0362],\n",
      "         [-1.0000,  0.0362,  0.0290],\n",
      "         [-1.0000,  0.0362,  0.0217],\n",
      "         [-1.0000,  0.0362,  0.0145],\n",
      "         [-1.0000,  0.0362,  0.0072],\n",
      "         [-1.0000,  0.0362,  0.0000],\n",
      "         [-1.0000,  0.0362, -0.0072],\n",
      "         [-1.0000,  0.0362, -0.0145],\n",
      "         [-1.0000,  0.0362, -0.0217],\n",
      "         [-1.0000,  0.0362, -0.0290]],\n",
      "\n",
      "        [[-1.0000,  0.0290,  0.0362],\n",
      "         [-1.0000,  0.0290,  0.0290],\n",
      "         [-1.0000,  0.0290,  0.0217],\n",
      "         [-1.0000,  0.0290,  0.0145],\n",
      "         [-1.0000,  0.0290,  0.0072],\n",
      "         [-1.0000,  0.0290,  0.0000],\n",
      "         [-1.0000,  0.0290, -0.0072],\n",
      "         [-1.0000,  0.0290, -0.0145],\n",
      "         [-1.0000,  0.0290, -0.0217],\n",
      "         [-1.0000,  0.0290, -0.0290]],\n",
      "\n",
      "        [[-1.0000,  0.0217,  0.0362],\n",
      "         [-1.0000,  0.0217,  0.0290],\n",
      "         [-1.0000,  0.0217,  0.0217],\n",
      "         [-1.0000,  0.0217,  0.0145],\n",
      "         [-1.0000,  0.0217,  0.0072],\n",
      "         [-1.0000,  0.0217,  0.0000],\n",
      "         [-1.0000,  0.0217, -0.0072],\n",
      "         [-1.0000,  0.0217, -0.0145],\n",
      "         [-1.0000,  0.0217, -0.0217],\n",
      "         [-1.0000,  0.0217, -0.0290]],\n",
      "\n",
      "        [[-1.0000,  0.0145,  0.0362],\n",
      "         [-1.0000,  0.0145,  0.0290],\n",
      "         [-1.0000,  0.0145,  0.0217],\n",
      "         [-1.0000,  0.0145,  0.0145],\n",
      "         [-1.0000,  0.0145,  0.0072],\n",
      "         [-1.0000,  0.0145,  0.0000],\n",
      "         [-1.0000,  0.0145, -0.0072],\n",
      "         [-1.0000,  0.0145, -0.0145],\n",
      "         [-1.0000,  0.0145, -0.0217],\n",
      "         [-1.0000,  0.0145, -0.0290]],\n",
      "\n",
      "        [[-1.0000,  0.0072,  0.0362],\n",
      "         [-1.0000,  0.0072,  0.0290],\n",
      "         [-1.0000,  0.0072,  0.0217],\n",
      "         [-1.0000,  0.0072,  0.0145],\n",
      "         [-1.0000,  0.0072,  0.0072],\n",
      "         [-1.0000,  0.0072,  0.0000],\n",
      "         [-1.0000,  0.0072, -0.0072],\n",
      "         [-1.0000,  0.0072, -0.0145],\n",
      "         [-1.0000,  0.0072, -0.0217],\n",
      "         [-1.0000,  0.0072, -0.0290]],\n",
      "\n",
      "        [[-1.0000,  0.0000,  0.0362],\n",
      "         [-1.0000,  0.0000,  0.0290],\n",
      "         [-1.0000,  0.0000,  0.0217],\n",
      "         [-1.0000,  0.0000,  0.0145],\n",
      "         [-1.0000,  0.0000,  0.0072],\n",
      "         [-1.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.0000, -0.0072],\n",
      "         [-1.0000,  0.0000, -0.0145],\n",
      "         [-1.0000,  0.0000, -0.0217],\n",
      "         [-1.0000,  0.0000, -0.0290]],\n",
      "\n",
      "        [[-1.0000, -0.0072,  0.0362],\n",
      "         [-1.0000, -0.0072,  0.0290],\n",
      "         [-1.0000, -0.0072,  0.0217],\n",
      "         [-1.0000, -0.0072,  0.0145],\n",
      "         [-1.0000, -0.0072,  0.0072],\n",
      "         [-1.0000, -0.0072,  0.0000],\n",
      "         [-1.0000, -0.0072, -0.0072],\n",
      "         [-1.0000, -0.0072, -0.0145],\n",
      "         [-1.0000, -0.0072, -0.0217],\n",
      "         [-1.0000, -0.0072, -0.0290]],\n",
      "\n",
      "        [[-1.0000, -0.0145,  0.0362],\n",
      "         [-1.0000, -0.0145,  0.0290],\n",
      "         [-1.0000, -0.0145,  0.0217],\n",
      "         [-1.0000, -0.0145,  0.0145],\n",
      "         [-1.0000, -0.0145,  0.0072],\n",
      "         [-1.0000, -0.0145,  0.0000],\n",
      "         [-1.0000, -0.0145, -0.0072],\n",
      "         [-1.0000, -0.0145, -0.0145],\n",
      "         [-1.0000, -0.0145, -0.0217],\n",
      "         [-1.0000, -0.0145, -0.0290]],\n",
      "\n",
      "        [[-1.0000, -0.0217,  0.0362],\n",
      "         [-1.0000, -0.0217,  0.0290],\n",
      "         [-1.0000, -0.0217,  0.0217],\n",
      "         [-1.0000, -0.0217,  0.0145],\n",
      "         [-1.0000, -0.0217,  0.0072],\n",
      "         [-1.0000, -0.0217,  0.0000],\n",
      "         [-1.0000, -0.0217, -0.0072],\n",
      "         [-1.0000, -0.0217, -0.0145],\n",
      "         [-1.0000, -0.0217, -0.0217],\n",
      "         [-1.0000, -0.0217, -0.0290]],\n",
      "\n",
      "        [[-1.0000, -0.0290,  0.0362],\n",
      "         [-1.0000, -0.0290,  0.0290],\n",
      "         [-1.0000, -0.0290,  0.0217],\n",
      "         [-1.0000, -0.0290,  0.0145],\n",
      "         [-1.0000, -0.0290,  0.0072],\n",
      "         [-1.0000, -0.0290,  0.0000],\n",
      "         [-1.0000, -0.0290, -0.0072],\n",
      "         [-1.0000, -0.0290, -0.0145],\n",
      "         [-1.0000, -0.0290, -0.0217],\n",
      "         [-1.0000, -0.0290, -0.0290]]])\n"
     ]
    }
   ],
   "source": [
    "def get_rays(H, W, focal, pose):\n",
    "  \"\"\"\n",
    "  This function generates camera rays for each pixel in an image. It calculates the origin and direction of rays\n",
    "  based on the camera's intrinsic parameters (focal length) and extrinsic parameters (pose).\n",
    "  The rays are generated in world coordinates, which is crucial for the NeRF rendering process.\n",
    "\n",
    "  Parameters:\n",
    "  H (int): Height of the image in pixels.\n",
    "  W (int): Width of the image in pixels.\n",
    "  focal (float): Focal length of the camera.\n",
    "  pose (torch.Tensor): Camera pose matrix of size 4x4.\n",
    "\n",
    "  Returns:\n",
    "  tuple: A tuple containing two elements:\n",
    "      rays_o (torch.Tensor): Origins of the rays in world coordinates.\n",
    "      rays_d (torch.Tensor): Directions of the rays in world coordinates.\n",
    "  \"\"\"\n",
    "  # Create a meshgrid of image coordinates (i, j) for each pixel in the image.\n",
    "  i, j = torch.meshgrid(\n",
    "      torch.arange(W, dtype=torch.float32),\n",
    "      torch.arange(H, dtype=torch.float32)\n",
    "  )\n",
    "  # print(\"i is:\", i)\n",
    "  # print(\"j is:\", j)\n",
    "  i = i.t()\n",
    "  j = j.t()\n",
    "  # print(\"i is:\", i)\n",
    "  # print(\"j is:\", j)\n",
    "\n",
    "  # Calculate the direction vectors for each ray originating from the camera center.\n",
    "  # We assume the camera looks towards -z.\n",
    "  # The coordinates are normalized with respect to the focal length.\n",
    "  dirs = torch.stack(\n",
    "      [(i - W * 0.5) / focal,\n",
    "        -(j - H * 0.5) / focal,\n",
    "        -torch.ones_like(i)], -1\n",
    "      )\n",
    "\n",
    "  # Transform the direction vectors (dirs) from camera coordinates to world coordinates.\n",
    "  # This is done using the rotation part (first 3 columns) of the pose matrix.\n",
    "  rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1) #[100,100,1,3] * [3,3] = [100,100,3,3] after sum(,-1) --> [100,100,3]\n",
    "\n",
    "  # The ray origins (rays_o) are set to the camera position, given by the translation part (last column) of the pose matrix.\n",
    "  # The position is expanded to match the shape of rays_d for broadcasting.\n",
    "  rays_o = pose[:3, -1].expand(rays_d.shape)\n",
    "\n",
    "  # Return the origins and directions of the rays.\n",
    "  return rays_o, rays_d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i, j = torch.meshgrid(\n",
    "    torch.arange(W, dtype=torch.float32),\n",
    "    torch.arange(H, dtype=torch.float32)\n",
    ")\n",
    "# print(\"i is:\", i)\n",
    "# print(\"j is:\", j)\n",
    "i = i.t()\n",
    "j = j.t()\n",
    "# print(\"i is:\", i)\n",
    "# print(\"j is:\", j)\n",
    "\n",
    "# Calculate the direction vectors for each ray originating from the camera center.\n",
    "# We assume the camera looks towards -z.\n",
    "# The coordinates are normalized with respect to the focal length.\n",
    "dirs = torch.stack(\n",
    "    [(i - W * 0.5) / focal,\n",
    "      -(j - H * 0.5) / focal,\n",
    "      -torch.ones_like(i)], -1\n",
    "    )\n",
    "# print(dirs.shape)\n",
    "# print(dirs)\n",
    "rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)  \n",
    "print(rays_d.shape)\n",
    "print(rays_d)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RSS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
